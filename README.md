# üìä Data Engineering Project - [Nome do Projeto]

Este projeto foi desenvolvido para processar e analisar dados do The Movie Database(TMDB). Utiliza Apache Spark, Airflow e PostgreSQL para transformar dados brutos em insights acion√°veis.

üìå **Principais funcionalidades**:
- Coleta de dados via API.
- Processamento com Spark e armazenamento em camadas Bronze, Silver e Gold.
- Orquestra√ß√£o de pipelines com Airflow.
- Visualiza√ß√£o de dados com Metabase.

## üèóÔ∏è Arquitetura

O projeto segue a arquitetura de Data Lakehouse, organizando os dados em tr√™s camadas:

1. **Bronze**: Dados brutos armazenados no MinIO.
2. **Silver**: Dados limpos e transformados com Spark.
3. **Gold**: Dados prontos para an√°lise no PostgreSQL.

üöÄ **Tecnologias Utilizadas**:
- **Docker**: Gerenciamento dos servi√ßos do projeto.
- **Apache Airflow**: Orquestra√ß√£o dos pipelines de dados.
- **Apache Spark**: Processamento e transforma√ß√£o dos dados.
- **PostgreSQL**: Banco de dados para armazenamento e consultas anal√≠ticas.
- **MinIO**: Armazenamento de dados como Data Lake.
- **Metabase**: Visualiza√ß√£o de dados e dashboards.

### üî• **Diagrama**
![Arquitetura](docs/arquitetura.png)  <!-- Adicione uma imagem da arquitetura, se poss√≠vel -->
## üìÇ Estrutura de Diret√≥rios

```
üì¶ projeto-de-engenharia-de-dados
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dag_processor_manager/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scheduler/
‚îú‚îÄ‚îÄ data_lake/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îú‚îÄ‚îÄ gold/
‚îú‚îÄ‚îÄ env/
‚îú‚îÄ‚îÄ metabase/
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îú‚îÄ‚îÄ postgres/
‚îÇ   ‚îú‚îÄ‚îÄ backups/
‚îú‚îÄ‚îÄ spark/
‚îÇ   ‚îú‚îÄ‚îÄ jobs/
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îî‚îÄ‚îÄ README.md
```

## üöÄ Como Rodar o Projeto

### 1Ô∏è‚É£ **Pr√©-requisitos**
Antes de iniciar, certifique-se de ter os seguintes softwares instalados:
- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- [Python 3.9+](https://www.python.org/)
- [API](https://www.themoviedb.org/settings/api)

### 2Ô∏è‚É£ **Passos para execu√ß√£o**

Clone o reposit√≥rio:
```bash
git clone https://github.com/seu-usuario/seu-repositorio.git
cd seu-repositorio

Ajuste o arquivo "api.env" com o seu token de autentica√ß√£o e sua chave de acesso. 

docker-compose up -d
```

Acesse os servi√ßos:

- Airflow: http://localhost:8080 (user: admin, senha: admin)
- MinIO: http://localhost:9001 (user: minioadmin, senha: minioadmin)
- Metabase: http://localhost:3000
- Jupyter http://localhost:8888


# Configura√ß√£o MinIO e Airflow

### 1Ô∏è‚É£ Acesse o webserver do MinIO e crie sua **Access Key**  

![MinIO Webserver](https://prnt.sc/1M4AMeExxxfN)

---

### 2Ô∏è‚É£ Acesse o webserver do Airflow  
- V√° at√© o menu **Variables** e adicione sua **Access Key** e **Password**, e configure o alerta de e-mail:

| **Key**                | **Value**         |
|------------------------|-------------------|
| `MINIO_ROOT_USER`      | *Sua Access Key*      |
| `MINIO_ROOT_PASSWORD`  | *Sua Access Password* |


> ‚ö†Ô∏è **Aten√ß√£o:** Altere o campo Value para os seus dados

### 3Ô∏è‚É£ Configura√ß√£o de alerta de falha por e-mail  

Abra o arquivo `smtp.env` no diret√≥rio `env` e altere os seguintes campos:  

| **Campo**                        | **Valor**                      |
|--------------------------------|--------------------------------|
| `AIRFLOW__SMTP__SMTP_USER=`     | *Seu e-mail*                   |
| `AIRFLOW__SMTP__SMTP_PASSWORD=` | *Sua senha app do e-mail*      |
| `AIRFLOW__SMTP__SMTP_MAIL_FROM=`| *Seu e-mail*                   |

---

### 3Ô∏è‚É£ Configura√ß√£o do Spark no Airflow  
- Acesse o menu **Connections** e edite a conex√£o `spark_default`.  
- Altere o **Host** de `YARN` para:  local[*]

![Airflow Connections](https://prnt.sc/ksDBKW21N66Z)

---

### **5Ô∏è‚É£ Pipelines de Dados**
Descreva os principais fluxos de dados no projeto.  

```md
## ‚öôÔ∏è Pipelines de Dados

1Ô∏è‚É£ **Ingest√£o**: Os dados s√£o coletados via API e armazenados no MinIO (camada Bronze).  
2Ô∏è‚É£ **Processamento**: O Apache Spark processa os dados e os salva na camada Silver.  
3Ô∏è‚É£ **Armazenamento**: Dados transformados s√£o carregados no PostgreSQL (camada Gold).  
4Ô∏è‚É£ **Orquestra√ß√£o**: O Airflow gerencia a execu√ß√£o autom√°tica dos pipelines.  

üõ† **Principais DAGs do Airflow**:
- `ingestao_dados.py`: Coleta os dados brutos da API.
- `processamento_spark.py`: Transforma os dados na camada Silver.
- `carga_postgres.py`: Carrega os dados finais no PostgreSQL.
```

## üìä Exemplos de Uso

Ap√≥s rodar o pipeline, voc√™ pode consultar os dados processados no PostgreSQL:

```sql
SELECT * FROM gold.tabela_principal LIMIT 10;
```
Ou usar PySpark para carregar os dados:
```
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Projeto").getOrCreate()
df = spark.read.parquet("data_lake/gold/dataset_final.parquet")
df.show()
```

---

### **7Ô∏è‚É£ Contribui√ß√£o e Contato** 

```md
## ü§ù Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Para contribuir:
1. Fa√ßa um fork do reposit√≥rio.
2. Crie uma nova branch (`git checkout -b minha-feature`).
3. Commit suas mudan√ßas (`git commit -m "Adicionei nova feature"`).
4. Envie um pull request.
```
## üì¨ Contato
D√∫vidas ou sugest√µes? Me encontre em:
üìß **Email:** joathan94@yahoo.com   
üìò **LinkedIn:** [Joathan V Grasel](https://www.linkedin.com/in/jgrasel/)

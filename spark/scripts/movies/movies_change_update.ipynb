{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9af32-4a41-4273-91d8-a1de5e366ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import col, sum, array, when\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea31425e-970b-4791-9ff2-b414b27b69e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2fbad685-fd45-4d26-b338-6462abbde63c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.1!hadoop-aws.jar (7259ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.901!aws-java-sdk-bundle.jar (251821ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (1057ms)\n",
      ":: resolution report :: resolve 11122ms :: artifacts dl 260146ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2fbad685-fd45-4d26-b338-6462abbde63c\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (189078kB/185ms)\n",
      "25/02/28 16:53:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Movies_change_update\") \\\n",
    "    .master(\"spark://spark:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\")  \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.901\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca91b40c-a986-4329-86c9-39e9c0d8a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_null_counts(df, df_name):\n",
    "    \"\"\"Conta a quantidade de valores nulos por coluna em um DataFrame\"\"\"\n",
    "    logger.info(f\"Verificando nulos em {df_name}...\")\n",
    "    null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).collect()[0]\n",
    "    for column in df.columns:\n",
    "        logger.info(f\"Coluna {column}: {null_counts[column]} nulos\")\n",
    "\n",
    "def handle_nulls(df):\n",
    "    \"\"\"Realiza o tratamento de nulos para todas as colunas\"\"\"\n",
    "    logger.info(\"Iniciando tratamento de nulos...\")\n",
    "    \n",
    "    # Remove registros com ID nulo\n",
    "    initial_count = df.count()\n",
    "    df = df.filter(col(\"id\").isNotNull())\n",
    "    logger.info(f\"Removidos {initial_count - df.count()} registros com ID nulo\")\n",
    "\n",
    "    # Remove registros com Title nulo\n",
    "    initial_count = df.count()\n",
    "    df = df.filter(col(\"title\").isNotNull())\n",
    "    logger.info(f\"Removidos {initial_count - df.count()} registros com titulo nulo\")\n",
    "\n",
    "    # Preenche valores nulos para cada coluna\n",
    "    fill_values = {\n",
    "        'overview': 'Sem informação.',\n",
    "        'release_date': '1900-01-01',\n",
    "        'vote_average': '0.0'\n",
    "    }\n",
    "    \n",
    "    df = df.na.fill(fill_values)\n",
    "    \n",
    "    # Trata array vazio para genre_ids\n",
    "    df = df.withColumn(\"genre_ids\", \n",
    "        when(col(\"genre_ids\").isNull(), array().cast(ArrayType(IntegerType())))\n",
    "        .otherwise(col(\"genre_ids\")))\n",
    "    \n",
    "    logger.info(\"Tratamento de nulos concluído\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95c420-b990-4194-b447-0537f9825a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logger.info(\"Iniciando processamento...\")\n",
    "    \n",
    "    # Leitura dos dados\n",
    "    logger.info(\"Lendo dados da camada bronze...\")\n",
    "    df_change = spark.read.parquet(\"s3a://bronze/movies_changes/\")\n",
    "    logger.info(\"Lendo dados da camada silver...\")\n",
    "    df_70_26 = spark.read.parquet(\"s3a://silver/movies_70_26/\").cache()\n",
    "    \n",
    "    # Log inicial de nulos\n",
    "    log_null_counts(df_change, \"Raw Changes\")\n",
    "    log_null_counts(df_70_26, \"Raw Silver\")\n",
    "    \n",
    "    # Tratamento de nulos\n",
    "    logger.info(\"Processando dados da camada bronze...\")\n",
    "    df_change = handle_nulls(df_change)\n",
    "    \n",
    "    logger.info(\"Processando dados da camada silver...\")\n",
    "    df_70_26 = handle_nulls(df_70_26)\n",
    "    \n",
    "    # Log pós-tratamento\n",
    "    log_null_counts(df_change, \"Processed Changes\")\n",
    "    log_null_counts(df_70_26, \"Processed Silver\")\n",
    "\n",
    "    # Processamento principal\n",
    "    logger.info(\"Realizando join e merge...\")\n",
    "    df_filtered = df_70_26.join(df_change, on=\"id\", how=\"left_anti\")\n",
    "    df_updated = df_filtered.unionByName(df_change)\n",
    "    \n",
    "    # Verificação\n",
    "    logger.info(f\"Registros originais: {df_70_26.count()}\")\n",
    "    logger.info(f\"Registros de mudança: {df_change.count()}\")\n",
    "    logger.info(f\"Registros atualizados: {df_updated.count()}\")\n",
    "    \n",
    "    # Verificação final de nulos\n",
    "    log_null_counts(df_updated, \"Dataset Final\")\n",
    "    df_updated.show(10, truncate=False)\n",
    "\n",
    "    # Escrita dos dados\n",
    "    logger.info(\"Escrevendo dados atualizados...\")\n",
    "    df_updated.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"s3a://silver/movies_70_26/\")\n",
    "    \n",
    "    # Verificação final\n",
    "    logger.info(\"Verificando escrita...\")\n",
    "    df_read = spark.read.parquet(\"s3a://silver/movies_70_26/\")\n",
    "    logger.info(f\"Total de registros escritos: {df_read.count()}\")\n",
    "    df_read.show(5, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro durante o processamento: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    logger.info(\"Encerrando sessão Spark...\")\n",
    "    spark.stop()\n",
    "\n",
    "logger.info(\"Processo concluído com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

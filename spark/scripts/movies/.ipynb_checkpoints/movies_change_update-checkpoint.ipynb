{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9af32-4a41-4273-91d8-a1de5e366ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import calendar\n",
    "import logging\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import col, sum, array, when\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31425e-970b-4791-9ff2-b414b27b69e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Movies_change_update\") \\\n",
    "    .master(\"spark://spark:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\")  \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.901\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91b40c-a986-4329-86c9-39e9c0d8a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_null_counts(df, df_name):\n",
    "    \"\"\"Conta a quantidade de valores nulos por coluna em um DataFrame\"\"\"\n",
    "    logger.info(f\"Verificando nulos em {df_name}...\")\n",
    "    null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).collect()[0]\n",
    "    for column in df.columns:\n",
    "        logger.info(f\"Coluna {column}: {null_counts[column]} nulos\")\n",
    "\n",
    "def handle_nulls(df):\n",
    "    \"\"\"Realiza o tratamento de nulos para todas as colunas\"\"\"\n",
    "    logger.info(\"Iniciando tratamento de nulos...\")\n",
    "    \n",
    "    # Remove registros com ID nulo\n",
    "    initial_count = df.count()\n",
    "    df = df.filter(col(\"id\").isNotNull())\n",
    "    logger.info(f\"Removidos {initial_count - df.count()} registros com ID nulo\")\n",
    "\n",
    "    # Remove registros com Title nulo\n",
    "    initial_count = df.count()\n",
    "    df = df.filter(col(\"title\").isNotNull())\n",
    "    logger.info(f\"Removidos {initial_count - df.count()} registros com titulo nulo\")\n",
    "\n",
    "    # Preenche valores nulos para cada coluna\n",
    "    fill_values = {\n",
    "        'overview': 'Sem informação.',\n",
    "        'release_date': '1900-01-01',\n",
    "        'vote_average': '0.0'\n",
    "    }\n",
    "    \n",
    "    df = df.na.fill(fill_values)\n",
    "    \n",
    "    # Trata array vazio para genre_ids\n",
    "    df = df.withColumn(\"genre_ids\", \n",
    "        when(col(\"genre_ids\").isNull(), array().cast(ArrayType(IntegerType())))\n",
    "        .otherwise(col(\"genre_ids\")))\n",
    "    \n",
    "    logger.info(\"Tratamento de nulos concluído\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95c420-b990-4194-b447-0537f9825a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logger.info(\"Iniciando processamento...\")\n",
    "    \n",
    "    # Leitura dos dados\n",
    "    logger.info(\"Lendo dados da camada bronze...\")\n",
    "    df_change = spark.read.parquet(\"s3a://bronze/movies_changes/\")\n",
    "    logger.info(\"Lendo dados da camada silver...\")\n",
    "    df_70_26 = spark.read.parquet(\"s3a://silver/movies_70_26/\").cache()\n",
    "    \n",
    "    # Log inicial de nulos\n",
    "    log_null_counts(df_change, \"Raw Changes\")\n",
    "    log_null_counts(df_70_26, \"Raw Silver\")\n",
    "    \n",
    "    # Tratamento de nulos\n",
    "    logger.info(\"Processando dados da camada bronze...\")\n",
    "    df_change = handle_nulls(df_change)\n",
    "    \n",
    "    logger.info(\"Processando dados da camada silver...\")\n",
    "    df_70_26 = handle_nulls(df_70_26)\n",
    "    \n",
    "    # Log pós-tratamento\n",
    "    log_null_counts(df_change, \"Processed Changes\")\n",
    "    log_null_counts(df_70_26, \"Processed Silver\")\n",
    "\n",
    "    # Processamento principal\n",
    "    logger.info(\"Realizando join e merge...\")\n",
    "    df_filtered = df_70_26.join(df_change, on=\"id\", how=\"left_anti\")\n",
    "    df_updated = df_filtered.unionByName(df_change)\n",
    "    \n",
    "    # Verificação\n",
    "    logger.info(f\"Registros originais: {df_70_26.count()}\")\n",
    "    logger.info(f\"Registros de mudança: {df_change.count()}\")\n",
    "    logger.info(f\"Registros atualizados: {df_updated.count()}\")\n",
    "    \n",
    "    # Verificação final de nulos\n",
    "    log_null_counts(df_updated, \"Dataset Final\")\n",
    "    df_updated.show(10, truncate=False)\n",
    "\n",
    "    # Escrita dos dados\n",
    "    logger.info(\"Escrevendo dados atualizados...\")\n",
    "    df_updated.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"s3a://silver/movies_70_26/\")\n",
    "    \n",
    "    # Verificação final\n",
    "    logger.info(\"Verificando escrita...\")\n",
    "    df_read = spark.read.parquet(\"s3a://silver/movies_70_26/\")\n",
    "    logger.info(f\"Total de registros escritos: {df_read.count()}\")\n",
    "    df_read.show(5, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro durante o processamento: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    logger.info(\"Encerrando sessão Spark...\")\n",
    "    spark.stop()\n",
    "\n",
    "logger.info(\"Processo concluído com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

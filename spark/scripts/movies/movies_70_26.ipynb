{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e9af32-4a41-4273-91d8-a1de5e366ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, IntegerType  # Importação adicionada\n",
    "from pyspark.sql.functions import col, sum, array, when\n",
    "from dotenv import load_dotenv\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31425e-970b-4791-9ff2-b414b27b69e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-eee3714a-6da3-4754-a68e-1cad88c15089;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 224ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-eee3714a-6da3-4754-a68e-1cad88c15089\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "25/02/28 17:06:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Movies_70_26\") \\\n",
    "    .master(\"spark://spark:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\")  \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.getenv(\"AWS_SECRET_ACCESS_KEY\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.901\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8abbbfc6-053d-4ebe-845f-acc4384617cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_null_counts(df, df_name):\n",
    "    \"\"\"Conta a quantidade de valores nulos por coluna em um DataFrame\"\"\"\n",
    "    logger.info(f\"Verificando nulos em {df_name}...\")\n",
    "    null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).collect()[0]\n",
    "    for column in df.columns:\n",
    "        logger.info(f\"Coluna {column}: {null_counts[column]} nulos\")\n",
    "\n",
    "def handle_nulls(df):\n",
    "    \"\"\"Realiza o tratamento de nulos para todas as colunas\"\"\"\n",
    "    logger.info(\"Iniciando tratamento de nulos...\")\n",
    "    \n",
    "    # Remove registros com ID nulo\n",
    "    initial_count = df.count()\n",
    "    df = df.filter(col(\"id\").isNotNull())\n",
    "    logger.info(f\"Removidos {initial_count - df.count()} registros com ID nulo\")\n",
    "\n",
    "    # Remove registros com Title nulo\n",
    "    initial_count = df.count()\n",
    "    df = df.filter(col(\"title\").isNotNull())\n",
    "    logger.info(f\"Removidos {initial_count - df.count()} registros com titulo nulo\")\n",
    "\n",
    "    # Preenche valores nulos para cada coluna\n",
    "    fill_values = {\n",
    "        'overview': 'Sem informação.',\n",
    "        'release_date': '1900-01-01',\n",
    "        'vote_average': '0.0'\n",
    "    }\n",
    "    \n",
    "    df = df.na.fill(fill_values)\n",
    "    \n",
    "    # Trata array vazio para genre_ids\n",
    "    df = df.withColumn(\"genre_ids\", \n",
    "        when(col(\"genre_ids\").isNull(), array().cast(ArrayType(IntegerType())))\n",
    "        .otherwise(col(\"genre_ids\")))\n",
    "    \n",
    "    logger.info(\"Tratamento de nulos concluído\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f6cd048-5b5e-43ac-a0b3-180c69acb7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Iniciando processo de ingestão Bronze -> Silver...\n",
      "INFO:__main__:Lendo arquivos da camada bronze...\n",
      "25/02/28 17:06:13 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "INFO:__main__:Total de registros brutos: 748896                                 \n",
      "INFO:__main__:Verificando nulos em Dataset Bruto...\n",
      "INFO:__main__:Coluna id: 0 nulos                                                \n",
      "INFO:__main__:Coluna title: 0 nulos\n",
      "INFO:__main__:Coluna overview: 0 nulos\n",
      "INFO:__main__:Coluna release_date: 0 nulos\n",
      "INFO:__main__:Coluna vote_average: 0 nulos\n",
      "INFO:__main__:Coluna genre_ids: 0 nulos\n",
      "INFO:__main__:Iniciando tratamento de nulos...\n",
      "INFO:__main__:Removidos 0 registros com ID nulo                                 \n",
      "INFO:__main__:Removidos 0 registros com titulo nulo                             \n",
      "INFO:__main__:Tratamento de nulos concluído\n",
      "INFO:__main__:Removendo duplicatas...\n",
      "INFO:__main__:Registros removidos por duplicidade: 0                            \n",
      "INFO:__main__:Verificando nulos em Dataset Processado...\n",
      "INFO:__main__:Coluna id: 0 nulos                                                \n",
      "INFO:__main__:Coluna title: 0 nulos\n",
      "INFO:__main__:Coluna overview: 0 nulos\n",
      "INFO:__main__:Coluna release_date: 0 nulos\n",
      "INFO:__main__:Coluna vote_average: 0 nulos\n",
      "INFO:__main__:Coluna genre_ids: 0 nulos\n",
      "INFO:__main__:Escrevendo na camada silver...\n",
      "INFO:__main__:Verificando escrita...                                            \n",
      "INFO:__main__:Total de registros escritos: 748896\n",
      "INFO:__main__:Encerrando sessão Spark...                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------+---------------+\n",
      "|id |title              |overview                                                                                                                                                                                                                                                                                                                                     |release_date|vote_average|genre_ids      |\n",
      "+---+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------+---------------+\n",
      "|3  |Shadows in Paradise|Nikander, a rubbish collector and would-be entrepreneur, finds his plans for success dashed when his business associate dies. One evening, he meets Ilona, a down-on-her-luck cashier, in a local supermarket. Falteringly, a bond begins to develop between them.                                                                           |1986-10-17  |7.3         |[35, 18, 10749]|\n",
      "|5  |Four Rooms         |It's Ted the Bellhop's first night on the job...and the hotel's very unusual guests are about to place him in some outrageous predicaments. It seems that this evening's room service is serving up one unbelievable happening after another.                                                                                                |1995-12-09  |5.9         |[35]           |\n",
      "|6  |Judgment Night     |Four young friends, while taking a shortcut en route to a local boxing match, witness a brutal murder which leaves them running for their lives.                                                                                                                                                                                             |1993-10-15  |6.47        |[28, 80, 53]   |\n",
      "|12 |Finding Nemo       |Nemo, an adventurous young clownfish, is unexpectedly taken from his Great Barrier Reef home to a dentist's office aquarium. It's up to his worrisome father Marlin and a friendly but forgetful fish Dory to bring Nemo home -- meeting vegetarian sharks, surfer dude turtles, hypnotic jellyfish, hungry seagulls, and more along the way.|2003-05-30  |7.817       |[16, 10751]    |\n",
      "|13 |Forrest Gump       |A man with a low IQ has accomplished great things in his life and been present during significant historic events—in each case, far exceeding what anyone imagined he could do. But despite all he has achieved, his one true love eludes him.                                                                                               |1994-06-23  |8.468       |[35, 18, 10749]|\n",
      "+---+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Ingestão concluída com sucesso!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logger.info(\"Iniciando processo de ingestão Bronze -> Silver...\")\n",
    "    \n",
    "    # Leitura dos dados\n",
    "    logger.info(\"Lendo arquivos da camada bronze...\")\n",
    "    df = spark.read.parquet(\"s3a://bronze/movie_*\")\n",
    "    \n",
    "    # Log inicial\n",
    "    logger.info(f\"Total de registros brutos: {df.count()}\")\n",
    "    log_null_counts(df, \"Dataset Bruto\")\n",
    "    \n",
    "    # Tratamento de nulos\n",
    "    df_clean = handle_nulls(df)\n",
    "    \n",
    "    # Deduplicação\n",
    "    logger.info(\"Removendo duplicatas...\")\n",
    "    initial_count = df_clean.count()\n",
    "    df_deduplicado = df_clean.dropDuplicates([\"id\"])\n",
    "    final_count = df_deduplicado.count()\n",
    "    \n",
    "    logger.info(f\"Registros removidos por duplicidade: {initial_count - final_count}\")\n",
    "    log_null_counts(df_deduplicado, \"Dataset Processado\")\n",
    "    \n",
    "    # Escrita dos dados\n",
    "    logger.info(\"Escrevendo na camada silver...\")\n",
    "    df_deduplicado.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"s3a://silver/movies_70_26/\")\n",
    "    \n",
    "    # Verificação final\n",
    "    logger.info(\"Verificando escrita...\")\n",
    "    df_read = spark.read.parquet(\"s3a://silver/movies_70_26/\")\n",
    "    logger.info(f\"Total de registros escritos: {df_read.count()}\")\n",
    "    df_read.show(5, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro durante o processamento: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    logger.info(\"Encerrando sessão Spark...\")\n",
    "    spark.stop()\n",
    "\n",
    "logger.info(\"Ingestão concluída com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
